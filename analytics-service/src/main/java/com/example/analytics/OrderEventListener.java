package com.example.analytics;
// ğŸ“¦ The package groups related code logically.
// In a large project, this helps structure components by domain (here, analytics service).

import org.apache.kafka.clients.consumer.ConsumerRecord;
// ğŸ“¨ Represents one message fetched from a Kafka topic (like an email with metadata).

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
// ğŸªµ Logging API â€“ used for structured runtime logs instead of println().
// Itâ€™s a facade (front-end) for different logging frameworks (Logback, Log4j, etc.).

import org.springframework.beans.factory.annotation.Value;
// ğŸ’¡ Lets you inject values from configuration files (application.yml / properties).
// Example: If config says app.topic.orders=orders.v1 â†’ this will inject "orders.v1" into a variable.

import org.springframework.kafka.annotation.KafkaListener;
// ğŸ§ Annotation that tells Spring Boot this method listens to Kafka messages automatically.
// It sets up background threads, polling loops, error handling, and deserialization.

import org.springframework.stereotype.Component;
// ğŸ§© Marks this class as a Spring-managed bean so itâ€™s auto-detected during component scanning.

@Component
// âœ… Spring will create one instance of this class at startup.
// Think of it as plugging in a â€œmessage receiverâ€ device when your service boots.

public class OrderEventListener {

    // ğŸ¤ Logger helps track message processing and debug issues.
    private static final Logger log = LoggerFactory.getLogger(OrderEventListener.class);

    // ğŸ§© The topic name is injected from configuration (externalized, not hardcoded).
    @Value("${app.topic.orders}")
    private String ordersTopic;

    // ğŸ§  @KafkaListener creates a background thread that subscribes to a Kafka topic.
    // It automatically polls messages and invokes this method for each message.
    @KafkaListener(
            topics = "#{@environment.getProperty('app.topic.orders')}",  // ğŸ‘‚ topic name from environment (dynamic config)
            groupId = "${app.consumer.group}",                           // ğŸ‘¥ consumer group ID for load balancing
            properties = {
                    "max.poll.records:100",                               // ğŸ§º Max messages to fetch per poll
                    "auto.offset.reset:earliest"                          // â® Start from earliest offset if no commit exists
            })
    public void onMessage(ConsumerRecord<String, String> record) {
        // ğŸ’¬ Kafka passes a record = single message + metadata (key, value, partition, offset)

        log.info(
                "ğŸ“¦ Received OrderEvent | key={} | partition={} | offset={} | payload={}",
                record.key(),        // ğŸ”‘ message key (usually symbol or orderId)
                record.partition(),  // ğŸ§­ which partition this message came from
                record.offset(),     // ğŸ“‘ sequential offset in that partition
                record.value()       // ğŸ§¾ the message itself (JSON string)
        );

        // âœ… Example output:
        // ğŸ“¦ Received OrderEvent | key=o-1001 | partition=0 | offset=15 | payload={"orderId":"o-1001",...}
    }
}
